---
title: "Tidy Collaborative Filtering: A Stock Recommender"
author: Christoph Scheuch
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

Recommender systems are a key component of our digital lifes, ranging from e-commerce, online advertisements, movie recommendations, or more generally all kind of product recommendations. A recommender system aims to efficiently deliver relevant information to users by automatically searching through a large volume of dynamically generated information to provide users with personalized content. In this blog post, we illustrate the concept of recommender systems in building a simple stock recommendation tool that relies on publicly available portfolios from the social trading platform wikifolio.com. The resulting recommender proposes stocks for investors who already have their own portfolio and look for new investment opportunities. The underlying assumption is that the wikifolio traders hold stock portfolios that can provide inspiration for other investors. 

wikifolio is the leading social trading platform in Europe, where anyone can create and manage virtual portfolios, which are called wikifolios. wikifolio Traders include full time investors and successful entrepreneurs, as well as experts from different sectors, portfolio managers and financial editors. All traders share their trading ideas through transparent wikifolios. The wikifolios are easy to track and replicate, by investing in the corresponding, collateralized wikifolio index certificate. As of writing, there are more than 30k published wikifolios of more than 9k traders, indicating a large diversity of available portfolios for training our recommender. 

There are essentially three types of recommender models: recommenders via collaborative filtering, recommenders via content-based filtering and hybrid recommenders (that mix the first two). In this blog post, I focus on the collaborative filtering approach as it requires no information other than portfolios and can provide fairly high precision with little complexity. Nonetheless, I first briefly describe the the recommender approaches and refer to [Ricci et al. (2011)](https://link.springer.com/book/10.1007/978-0-387-85820-3) for a comprehensive exposition. 

## Collaborative Filtering 

In collaborative filtering, recommendations are purely based on past interactions between users and items to produce new recommendations. The main notion is that past user-item interactions are sufficient to detect similar users or items. Broadly speaking, there are two subclasses of collaborative filtering: the memory-based approach which essentially searches nearest neighbors based on recorded transactions and is hence model-free; and the model-based approach where new representations of users and items are built based on some generative pre-estimated model. Theoretically speaking, the memory-based approach has a low bias (since there is no latent model assumed), but a high variance (since the recommendations change a lot in the nearest neighbor search). The model-based approach relies on a trained interaction model and has hence a relatively higher bias, but a lower variance, i.e., recommendations are more stable since they come out of a model. 

Advantages of collaborative filtering include: (i) no information about users or items is required; (ii) a high precision can be achieved with little data; (iii) the more interaction between users and items is available, the more recommendations become accurate. However, the disadvantages of collaborative filtering are: (i) it is impossible to make recommendations to a new user or recommend a new item (cold start problem); (ii) calculating recommendations for millions of users or items consumes a lot of computational power (scalability problem); (iii) if the number of items is large relative to the users and most users only have interacted with a small subset of all items, then the resulting representation has many zero interactions and might hence lead to computational difficulties (sparsity problem). 

## Content-Based Filering

Content-based filtering methods exploit information about users or items to create recommendations by building a model that relates available characteristics of users or items to each other. The recommendation problem is hence cast into a classification problem (user will like the item or not) or more generally a regression problem (which rating will user give an item). The classification problem can be item-centered by focusing on available user information and estimating a model for each item. If there are a lot of user-item interactions available, the resulting model is fairly robust, but it is less personalised (as it ignores user characteristics apart from interactions). The classification problem can also be user-centered by working with item features and estimating a model for each user. However, if a user only has few interactions then the resulting model becomes easily instable. Content-based filtering can also be neither user nor item-centered by stacking the two feature vectors and putting them into a neural network. 

The main advantage of content-based filtering is that it can make recommendations for new users without any interaction history or recommend new items to users. The disadvantages include: (i) training needs a lot of users and item examples for reliable results; (ii) tuning might be much harder in practice than collaborative filering; (iii) missing information might be a problem since there is no clear solution how to treat missingness in user or item characteristics.

## Hybrid Recommenders

Hybrid recommender systems combine both collaborative and content-based filtering to overcome the challenges of each individual approach. There are different hybridization techniques available, e.g., combining the scores of different components (weighted), choosing among different compontent (switching), following strict priority rules (cascading), presenting outputs from different components at the same time (mixed), etc.

# Training Collaborative Filtering Recommenders

Here is the list of recommenders we consider for the backtest:

* Random Items: the benchmark case because it just stupidly chooses random stocks from all possible choices.
* Popular Items: just recommended the most popular stocks to each user (measured by the number of wikifolios that hold the stock).
* Association Rules: each wikifolio and its portfolio is considered as a transaction. Association rule mining finds similar portfolios across all users (if a users has x, y and z in his/her portfolio, then he/she is X% likely of also including w). It can be fully pre-computed.
* Item-Based Filtering: the algorithm calculates a similarity matrix across stocks. Recommendations are then based on the list of most similar stocks to the ones the wikifolio already has in its portfolio. This approach is also quite efficient since the similarities are pre-computed and recommendations can be generated very fast.
* User-Based Filtering: the algorithm finds a neighborhood of similar wikifolios based on their portfolios for each wikifolio (for this exercise it is set to 100 most similar wikifolios). Recommendations are then based on what the most similar wikifolios have in their portfolio. 

## Data preparation 

```{r}
# TODO: check whether we can use wikifolio data and how to host it (maybe pin?)
wikifolio_portfolios <- read_rds("data/wikifolio_portfolios.rds") %>%
  select(wikifolio = WikifolioIsin,
         stock = UnderlyingIsin,
         quantity = UnderlyingQuantity,
         price = UnderlyingPrice,
         weight = UnderlyingPortfolioWeight) %>%
  mutate(in_portfolio = 1) %>%
  group_by(stock) %>%
  filter(n_distinct(wikifolio) > 1) %>%
  ungroup() %>%
  group_by(wikifolio) %>%
  filter(n_distinct(stock) > 1) %>%
  ungroup()
```

```{r}
# Convert long data to binary rating matrix where only non-NA values are stored
# explicitely and NA values are represented by dots
binary_rating_matrix <- wikifolio_portfolios %>%
  pivot_wider(id_cols = wikifolio,
              names_from = stock,
              values_from = in_portfolio,
              values_fill = list(in_portfolio = 0)) %>%
  select(-wikifolio) %>%
  as.matrix() %>%
  as("binaryRatingMatrix")
binary_rating_matrix
```


# Evaluation of recommenders

```{r}
# cross validation: split in to training and test data
# k-fold: k-times recursive estimation of cross validation
# given: -1 means that all but 1 randomly selected item is ignored for evaluation
scheme <- binary_rating_matrix %>%
  evaluationScheme(method = "cross",
                   k      = 5,
                   train  = 0.8,
                   given  = -1)
scheme
```

```{r}
recommenderRegistry$get_entries(dataType = "binaryRatingMatrix")

# specify set of algorithms
# popular = number of users who have the item in their profile
# support = how frequently the item set appears
# confidence = how often the rule has been found to be true
# nn = number of nearest neighbors
algorithms <- list(
  "Random Items"             = list(name  = "RANDOM",  param = NULL),
  "Popular Items"            = list(name  = "POPULAR", param = NULL),
  "Association Rules"        = list(name  = "AR", param = list(supp = 0.01, conf = 0.1)),
  "Item-Based Filtering"     = list(name  = "IBCF", param = list(k = 10)),
  "User-Based Filtering"     = list(name  = "UBCF", param = list(method = "Cosine", nn = 100))
)

# run algorithms
number_of_recommendations <- c(1, 3, 5, 10)
results <- evaluate(scheme,
                    algorithms,
                    type = "topNList",
                    progress = TRUE,
                    n = number_of_recommendations
)
results

# get evaluation metrics averaged of cross-validation folds
results_tbl <- results %>%
  avg() %>%
  map(as_tibble) %>%
  bind_rows(.id = "model")
```


For each recommender, we count the following numbers:

* True Negative (TN) = number of not predicted items that do not correspond to withheld items
* False Positive (FP) = number of incorrect predictions that do not correspond to withheld items
* False negative (FN) = number of not predicted items that correspond to withheld items
* True Positive (TP) = number of correct predictions that correspond to withheld items

The two figures below present the most common evaluation techniques for the performance of recommender algorithms in backtests.

## ROC curves

This approach comes from signal-detection and is called §Receiver Operating Characteristic" (ROC). The ROC-curve plots the algorithm’s probability of detection (TPR) agains the probability of false alarm (FPR).

* TPR = TP / (TP + FN) (i.e. share of true positive recommendations relative to all known portfolios)
* FPR = FP / (FP + TN) (i.e. share of incorrect recommendations relative to )

The bigger the area under the ROC curve, the better is the corresponding algorithm.

```{r eval = TRUE}
results_tbl %>%
  ggplot(aes(FPR, TPR, colour = model)) +
  geom_line() +
  geom_label(aes(label = n))  +
  labs(x = "False Positive Rate (FPR)",
       y = "True Positive Rate (TPR)",
       title = "Item-based filtering achieves the highest TPR given any FPR",
       colour = "Model") +
  theme_bw() +
  scale_y_continuous(labels = percent) +
  scale_x_continuous(labels = percent)
```


## Precision-recall curves

The following two measures are often used in information retrieval problems:

* Precision = TP / (TP + FP) (i.e. correctly recommended items relative to total recommended items)
* Recall = TP / (TP + FN) (i.e. correctly recommended items relative to total number of known useful recommendations)

The goal is to have a higher precision for any level of recall. In fact, there is trade-off between the two measures since high precision means low recall and vice-versa.

```{r eval = TRUE}
results_tbl %>%
  ggplot(aes(x = recall, y = precision, colour = model)) +
  geom_line() +
  geom_label(aes(label = n))  +
  labs(x = "Recall", y = "Precision",
       title = "Item-based filtering achieves higher precision than other models",
       colour = "Model") +
  theme_grey(base_size = 14) +
  theme_bw() +
  scale_y_continuous(labels = percent) +
  scale_x_continuous(labels = percent)
```


## Predictions for new users

```{r}
recommender <- Recommender(binary_rating_matrix, method = "IBCF")
```

```{r eval = TRUE}
sample_portfolio <- c("US67066G1040", "US0378331005")
sample_rating_matrix <- tibble(distinct(wikifolio_portfolios, stock)) %>%
  mutate(in_portfolio = if_else(stock %in% sample_portfolio, 1, 0)) %>%
  pivot_wider(names_from = stock,
              values_from = in_portfolio,
              values_fill = list(in_portfolio = 0)) %>%
  as.matrix() %>%
  as("binaryRatingMatrix")

prediction <- predict(recommender, sample_rating_matrix, n = 1)
as(prediction, "list")
```


